















































import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import math

# To ignore warnings
import warnings
warnings.filterwarnings("ignore")


df_yield = pd.read_csv("data/yield.csv")
df_pest = pd.read_csv("data/pesticides.csv")
df_rain = pd.read_csv("data/precipitation.csv")
df_temp = pd.read_csv("data/temperature.csv")








df_yield.head()





def find_singular_value_columns(df):
    singular_value_columns = []
    print("Singular value columns")
    for column in df.columns.tolist():
        uniques = df[column].unique()
        if len(uniques) == 1:
            singular_value_columns.append(column)
            print(f"{column}: {uniques}")
    return singular_value_columns


columns_to_drop = find_singular_value_columns(df_yield)





columns_to_drop.append("Year Code")
df_yield.drop(columns_to_drop, axis=1, inplace=True)





rename_map = {
    "Area Code (M49)": "Country Code",
    "Area": "Country",
    "Item Code (CPC)": "Crop Code",
    "Item": "Crop",
    "Value": "Yield (kg/ha)"
}
df_yield.rename(rename_map, axis=1, inplace=True)


df_yield.head()





df_pest.head()





columns_to_drop = find_singular_value_columns(df_pest)


columns_to_drop.append("Year Code")
df_pest.drop(columns_to_drop, axis=1, inplace=True)


rename_map = {
    "Area Code (M49)": "Country Code",
    "Area": "Country",
    "Value": "Pesticides (kg/ha)"
}
df_pest.rename(rename_map, axis=1, inplace=True)


df_pest.head()








df_rain.head()





df_rain.rename({"Entity": "Country"}, axis=1, inplace=True)


df_rain.drop(["Code"], axis=1, inplace=True)


df_rain.head()





df_temp.head()





df_temp.rename({"Annual Mean": "Annual Temperature"}, axis=1, inplace=True)


df_temp.drop(["5-yr smooth", "Code"], axis=1, inplace=True)


df_temp.head()








earliest_year_yield = df_yield["Year"].min()
earliest_year_pest = df_pest["Year"].min()
earliest_year_rain = df_rain["Year"].min()
earliest_year_temp = df_temp["Year"].min()
most_recent_earliest_year = np.max([earliest_year_yield, earliest_year_pest, earliest_year_rain, earliest_year_temp])
print(f"Most recent earliest year: {most_recent_earliest_year}")





latest_year_yield = df_yield["Year"].max()
latest_year_pest = df_pest["Year"].max()
latest_year_rain = df_rain["Year"].max()
latest_year_temp = df_temp["Year"].max()
earliest_latest_year = np.min([latest_year_yield, latest_year_pest, latest_year_rain, latest_year_temp])
print(f"Earliest latest year: {earliest_latest_year}")








countries_yield = df_yield["Country"].unique()
countries_pest = df_pest["Country"].unique()
countries_rain = df_rain["Country"].unique()
countries_temp = df_temp["Country"].unique()

countries = set(countries_yield)
countries_dfs = [countries_pest, countries_rain, countries_temp]


countries_diff = set()

for df in countries_dfs:
    diff = list(set(countries) - set(df)) + list(set(df) - set(countries))
    countries_diff.update(diff)
    countries.update(df)
    
countries_diff, len(countries_diff)





countries_replace = {
    "Brunei": "Brunei Darussalam",
    "Laos": "Lao People's Democratic Republic",
    "Vietnam": "Viet Nam"
}





df_rain["Country"] = df_rain["Country"].replace(countries_replace)
df_temp["Country"] = df_temp["Country"].replace(countries_replace)


countries_yield = df_yield["Country"].unique()
countries_pest = df_pest["Country"].unique()
countries_rain = df_rain["Country"].unique()
countries_temp = df_temp["Country"].unique()

countries = set(countries_yield)
countries_dfs = [countries_pest, countries_rain, countries_temp]

countries_diff = set()

for df in countries_dfs:
    diff = list(set(countries) - set(df)) + list(set(df) - set(countries))
    countries_diff.update(diff)
    countries.update(df)
    
countries_diff, len(countries_diff)





print(f"Yield {df_yield.shape}")
print(f"Pest {df_pest.shape}")
print(f"Rain {df_rain.shape}")
print(f"Temp {df_temp.shape}")





df_yield_merged = pd.merge(df_yield, df_pest, how="left", on=["Country", "Country Code", "Year"])
df_yield_merged.head()


df_yield_merged = pd.merge(df_yield_merged, df_rain, how="left", on=["Country", "Year"])
df_yield_merged.head()


df_yield_merged = pd.merge(df_yield_merged, df_temp, how="left", on=["Country", "Year"])
df_yield_merged.head()





df_yield_merged.shape








df_yield_merged = df_yield_merged[~df_yield_merged["Country"].isin(countries_diff)].reset_index(drop=True)
df_yield_merged














# official members of asean
asean_countries = [
    "Brunei Darussalam",
    "Cambodia",
    "Indonesia",
    "Lao People's Democratic Republic",
    "Malaysia",
    "Myanmar",
    "Philippines",
    "Singapore",
    "Thailand",
    "Viet Nam"
]
df_yield_asean = df_yield_merged[df_yield_merged["Country"].isin(asean_countries)].reset_index(drop=True)


df_yield_asean.head()





df_yield_asean.drop(["Country Code", "Crop Code"], axis=1, inplace=True)
df_yield_asean.head()


df_yield_asean.shape





df_yield_asean.info()








df_yield_asean.duplicated().sum()





df_yield_asean.describe()








len(df_yield_asean["Crop"].unique())








top10_crops = df_yield_asean[["Crop", "Yield (kg/ha)"]].groupby('Crop').mean().nlargest(10, "Yield (kg/ha)")


plt.figure(figsize=(16, 6))
plt.title("Top 10 producing crops in ASEAN")
sns.barplot(top10_crops, x="Crop", y="Yield (kg/ha)", hue="Crop")
plt.xticks(rotation=90)

plt.savefig('images/top10_crops_ASEAN.png', bbox_inches = 'tight')
plt.show()








avg_rice_yield_by_country = (
    df_yield_asean[df_yield_asean["Crop"] == "Rice"][["Country", "Yield (kg/ha)"]]
        .groupby(['Country'])
        .mean()
        .sort_values(by=["Yield (kg/ha)"], ascending=False)
)
avg_rice_yield_by_country


plt.figure(figsize=(16, 6))
plt.title("Avg. Rice Yield by Country in ASEAN")
sns.barplot(avg_rice_yield_by_country, x="Country", y="Yield (kg/ha)", hue="Country")
plt.xticks(rotation=90)

plt.savefig('images/avg_rice_yield_ASEAN.png', bbox_inches = 'tight')
plt.show()











metrics_columns = ["Yield (kg/ha)", "Pesticides (kg/ha)", "Annual precipitation", "Annual Temperature"]


corr = df_yield_asean[metrics_columns].corr()

plt.figure(figsize=(16, 6))
mask = np.triu(np.ones_like(corr, dtype=bool))
heatmap = sns.heatmap(corr, mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')
heatmap.set_title('General Correlation Heatmap', fontdict={'fontsize':18}, pad=16);

plt.savefig('images/crops_general_correlation.png', bbox_inches = 'tight')
plt.show()











df_yield_sugarcane = df_yield_asean[df_yield_asean["Crop"] == "Sugar cane"]
df_yield_sugarcane.head()


sugarcane_corr = df_yield_sugarcane[metrics_columns].corr()

plt.figure(figsize=(16, 6))
mask = np.triu(np.ones_like(sugarcane_corr, dtype=bool))
heatmap = sns.heatmap(sugarcane_corr, mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')
heatmap.set_title('Sugarcane Correlation Heatmap', fontdict={'fontsize':18}, pad=16);

plt.savefig('images/sugarcane_correlation.png', bbox_inches = 'tight')
plt.show()








df_yield_maize = df_yield_asean[df_yield_asean["Crop"] == "Maize (corn)"]
df_yield_maize.head()


maize_corr = df_yield_maize[metrics_columns].corr()

plt.figure(figsize=(16, 6))
mask = np.triu(np.ones_like(maize_corr, dtype=bool))
heatmap = sns.heatmap(maize_corr, mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')
heatmap.set_title('Maize Correlation Heatmap', fontdict={'fontsize':18}, pad=16);

plt.savefig('images/maize_correlation.png', bbox_inches = 'tight')
plt.show()








df_yield_rice = df_yield_asean[df_yield_asean["Crop"] == "Rice"]
df_yield_rice.head()


rice_corr = df_yield_rice[metrics_columns].corr()

plt.figure(figsize=(16, 6))
mask = np.triu(np.ones_like(rice_corr, dtype=bool))
heatmap = sns.heatmap(rice_corr, mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')
heatmap.set_title('Rice Correlation Heatmap', fontdict={'fontsize':18}, pad=16);

plt.savefig('images/rice_correlation.png', bbox_inches = 'tight')
plt.show()








df_yield_wheat = df_yield_asean[df_yield_asean["Crop"] == "Wheat"]
df_yield_wheat.head()


wheat_corr = df_yield_wheat[metrics_columns].corr()

plt.figure(figsize=(16, 6))
mask = np.triu(np.ones_like(wheat_corr, dtype=bool))
heatmap = sns.heatmap(wheat_corr, mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')
heatmap.set_title('Wheat Correlation Heatmap', fontdict={'fontsize':18}, pad=16);

plt.savefig('images/wheat_correlation.png', bbox_inches = 'tight')
plt.show()











feature_metrics_columns = ["Pesticides (kg/ha)", "Annual precipitation", "Annual Temperature"]


fig, axes = plt.subplots(1, 3)
fig.set_size_inches(20, 6)
fig.tight_layout()
for column, ax in zip(feature_metrics_columns, axes.flatten()):
    sns.boxplot(x=column, data=df_yield_asean, ax=ax)

plt.savefig('images/boxplots.png', bbox_inches = 'tight')
plt.show()





fig, axes = plt.subplots(1, 3)
fig.set_size_inches(20, 6)
fig.tight_layout()
for column, ax in zip(feature_metrics_columns, axes.flatten()):
    sns.histplot(x=column, data=df_yield_asean, kde=True, ax=ax)

plt.savefig('images/distributions.png', bbox_inches = 'tight')
plt.show()














df_yield_asean.drop(columns=["Year"], axis=1, inplace=True)
df_yield_asean.head()








splits = [0.05, 0.1, 0.2]





X = df_yield_asean.loc[:, df_yield_asean.columns != "Yield (kg/ha)"]
y = df_yield_asean["Yield (kg/ha)"]
X.shape, y.shape


from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=27)
X_train.shape, X_test.shape, y_train.shape, y_test.shape








len(X["Country"].unique()), len(X["Crop"].unique())








from sklearn.preprocessing import OneHotEncoder, TargetEncoder
from sklearn.compose import make_column_transformer


encoder = OneHotEncoder(handle_unknown='ignore')
encoder.fit(X_train[["Country"]])


ohe_values = encoder.transform(X_train[["Country"]])
encoded_values = pd.DataFrame(ohe_values.toarray(), columns=encoder.get_feature_names_out())
encoded_values.head()


X_train.drop(columns=["Country"], axis=1, inplace=True)
X_train.reset_index(drop=True, inplace=True)
X_train = pd.concat([X_train, encoded_values], axis=1)
X_train.head()


# apply first to training and testing data before training or testing
def one_hot_encode_transform(X, encoder, fit=False):
    if fit:
        encoder.fit(X[["Country"]])
    ohe_values = encoder.transform(X[["Country"]])
    encoded_values = pd.DataFrame(ohe_values.toarray(), columns=encoder.get_feature_names_out())
    X.drop(columns=["Country"], axis=1, inplace=True)
    X.reset_index(drop=True, inplace=True)
    X = pd.concat([X, encoded_values], axis=1)
    return X, encoder


target_enc = TargetEncoder(smooth="auto")
target_enc.fit(X_train[["Crop"]], y_train)


te_values = target_enc.transform(X_train[["Crop"]])
te_df = pd.DataFrame(te_values, columns=target_enc.get_feature_names_out())
te_df.head()


X_train.drop(columns=["Crop"], axis=1, inplace=True)
X_train.reset_index(drop=True, inplace=True)
X_train = pd.concat([X_train, te_df], axis=1)
X_train.head()


# apply second to training and testing data before training or testing
def target_encode_transform(X, encoder, y=None, fit=False):
    if fit:
        target_enc.fit(X[["Crop"]], y)
    te_values = target_enc.transform(X[["Crop"]])
    te_df = pd.DataFrame(te_values, columns=target_enc.get_feature_names_out())
    X.drop(columns=["Crop"], axis=1, inplace=True)
    X.reset_index(drop=True, inplace=True)
    X = pd.concat([X, te_df], axis=1)
    return X, encoder








from sklearn.preprocessing import MinMaxScaler


scaler = MinMaxScaler()
scaler.fit(X_train) 


scaled_values = scaler.transform(X_train)
X_train = pd.DataFrame(scaled_values, columns=X_train.columns)
X_train.head()


# apply third to training and testing data before training or testing
def scale_transform(X, scaler, fit=False):
    if fit:
        scaler.fit(X)
    scaled_values = scaler.transform(X)
    X = pd.DataFrame(scaled_values, columns=X.columns)
    return X, scaler











from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_validate
from sklearn.metrics import root_mean_squared_error, r2_score


models = []

models.append(('GBR', GradientBoostingRegressor(random_state=28)))
models.append(('RFR', RandomForestRegressor(random_state=28)))
models.append(('XGB', XGBRegressor(random_state=28)))
models.append(('SVR', SVR()))
models.append(('KNN', KNeighborsRegressor()))
models.append(('LR', Ridge(random_state=28)))








scoring = {'rmse' : 'neg_root_mean_squared_error', 'r2_score' : 'r2'}
training_3split_results = []

for split in splits:
    print(f"{int((1-split) * 100)}-{int(split * 100)} Split Training Accuracy")
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=27)

    training_results = []

    oh_encoder = OneHotEncoder(handle_unknown='ignore')
    target_encoder = TargetEncoder(smooth="auto")
    scaler = MinMaxScaler()

    X_train, oh_encoder = one_hot_encode_transform(X_train, oh_encoder, fit=True)
    X_train, target_encoder = target_encode_transform(X_train,  target_encoder, y=y_train, fit=True)
    X_train, scaler = scale_transform(X_train, scaler, fit=True)
    
    for name, model in models:
        scores = cross_validate(model, X_train, y_train, cv=5, scoring=scoring)
        rmse = np.mean(-scores["test_rmse"])
        r2 = np.mean(scores["test_r2_score"])
        training_results.append([name, rmse, r2])

    training_results_df = pd.DataFrame(training_results, columns=["Model", "RMSE Score", "R² Score"])
    training_3split_results.append(training_results_df)
    print(training_results_df)











testing_3split_results = []

for split in splits:
    print(f"{int((1-split) * 100)}-{int(split * 100)} Split Testing Accuracy")
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=27)

    predictions = {}
    testing_results = []

    oh_encoder = OneHotEncoder(handle_unknown='ignore')
    target_encoder = TargetEncoder(smooth="auto")
    scaler = MinMaxScaler()

    X_train, oh_encoder = one_hot_encode_transform(X_train, oh_encoder, fit=True)
    X_train, target_encoder = target_encode_transform(X_train, target_encoder, y=y_train, fit=True)
    X_train, scaler = scale_transform(X_train, scaler, fit=True)

    X_test, _ = one_hot_encode_transform(X_test, oh_encoder)
    X_test, _ = target_encode_transform(X_test,  target_encoder, y=y_test)
    X_test, _ = scale_transform(X_test, scaler)
    
    for name, model in models:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        predictions[name] = y_pred
        rmse = root_mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        testing_results.append([name, rmse, r2])
    
    testing_results_df = pd.DataFrame(testing_results, columns=["Model", "RMSE Score", "R² Score"])
    testing_3split_results.append(testing_results_df)
    print(testing_results_df)








training_results_df = training_3split_results[2]
testing_results_df = testing_3split_results[2]











from sklearn.model_selection import GridSearchCV

# Set up parameter grids for each model
param_grid = {
    'Gradiant Boosting Regressor': {
        'n_estimators': [50, 100, 200], 
        'learning_rate': [0.05, 0.02, 0.01, 0.1], 
        'max_depth': [5, 10, 15, 20, 25],
        'min_samples_split': [2, 5, 10, 15],
        'min_samples_leaf': [1, 5, 10, 20],
    }, 
    'Random Forest Regressor': {
        'n_estimators': [50, 100, 200], 
        'max_depth': [5, 10, 15, 20, 25],
        'min_samples_split': [2, 6, 12, 20],
        'min_samples_leaf': [1, 5, 10, 20],
    },  
    'XGBoost Regressor': {
        'max_depth': [5, 10, 15, 20, 25],
        'learning_rate': [0.05, 0.1, 0.3, 0.5],
        'subsample': [0.5, 0.7, 1],
    },
    'KNN Regressor': {
        'n_neighbors': [3, 5, 8, 12],
        'weights': ['uniform', 'distance'],
        'p': [1, 2],
    }
}

# Models
models = {
    "Gradiant Boosting Regressor": GradientBoostingRegressor(random_state=28),
    "Random Forest Regressor": RandomForestRegressor(random_state=28),
    "XGBoost Regressor": XGBRegressor(random_state=28),
    "KNN Regressor": KNeighborsRegressor()
}

# Store results
results = []

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=27)

oh_encoder = OneHotEncoder(handle_unknown='ignore')
target_encoder = TargetEncoder(smooth="auto")
scaler = MinMaxScaler()

X_train, oh_encoder = one_hot_encode_transform(X_train, oh_encoder, fit=True)
X_train, target_encodber = target_encode_transform(X_train, target_encoder, y=y_train, fit=True)
X_train, scaler = scale_transform(X_train, scaler, fit=True)

# Grid Search for hyperparameter tuning
for name, model in models.items():
    grid_search = GridSearchCV(model, param_grid[name], cv=5, scoring='r2', n_jobs = -1,verbose = 2)  # 5-fold CV for R²
    grid_search.fit(X_train, y_train)
    
    # Best parameters and best score
    best_params = grid_search.best_params_
    best_score = grid_search.best_score_
    
    # Store results
    results.append([name, best_params, best_score])

# Create a DataFrame to display the results
results_df = pd.DataFrame(results, columns=["Model", "Best Parameters", "Best CV R² Score"])
print("\nBest Hyperparameters and Scores from Grid Search:")
print(results)











gbr = GradientBoostingRegressor(random_state=28, learning_rate=0.1, max_depth=10, min_samples_leaf=5, min_samples_split=2, n_estimators=200)
rfr = RandomForestRegressor(random_state=28, max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100)
xgbr = XGBRegressor(random_state=28, learning_rate=0.05, max_depth=20, subsample=0.7)


# tuned cross validation for training accuracy

scoring = {'rmse' : 'neg_root_mean_squared_error', 'r2_score' : 'r2'}

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=27)

training_results = []

oh_encoder = OneHotEncoder(handle_unknown='ignore')
target_encoder = TargetEncoder(smooth="auto")
scaler = MinMaxScaler()

X_train, oh_encoder = one_hot_encode_transform(X_train, oh_encoder, fit=True)
X_train, target_encoder = target_encode_transform(X_train,  target_encoder, y=y_train, fit=True)
X_train, scaler = scale_transform(X_train, scaler, fit=True)

# GradientBoosting
gbr_scores = cross_validate(gbr, X_train, y_train, cv=5, scoring=scoring)
gbr_rmse = np.mean(-gbr_scores["test_rmse"])
gbr_r2 = np.mean(gbr_scores["test_r2_score"])
training_results.append(["Gradiant Boosting Regressor", gbr_rmse, gbr_r2])

# RandomForest
rfr_scores = cross_validate(rfr, X_train, y_train, cv=5, scoring=scoring)
rfr_rmse = np.mean(-rfr_scores["test_rmse"])
rfr_r2 = np.mean(rfr_scores["test_r2_score"])
training_results.append(["Random Forest Regressor", rfr_rmse, rfr_r2])

# XGBoost
xgbr_scores = cross_validate(xgbr, X_train, y_train, cv=5, scoring=scoring)
xgbr_rmse = np.mean(-xgbr_scores["test_rmse"])
xgbr_r2 = np.mean(xgbr_scores["test_r2_score"])
training_results.append(["XGBoost Regressor", xgbr_rmse, xgbr_r2])

tuned_training_results_df = pd.DataFrame(training_results, columns=["Model", "RMSE Score", "R² Score"])
print("80-20 Split Tuned Training Accuracy")
print(tuned_training_results_df)





# Improved training performance after tuning

improved_training_results_df = pd.DataFrame(data=[], columns=tuned_training_results_df.columns)
improved_training_results_df["Model"] = tuned_training_results_df["Model"]
improved_training_results_df["RMSE Score"] = tuned_training_results_df["RMSE Score"].sub(training_results_df["RMSE Score"])
improved_training_results_df["R² Score"] = tuned_training_results_df["R² Score"].sub(training_results_df["R² Score"])

print("Improved Training Results after tuning on 80-20 split")
print(improved_training_results_df)








X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=27)

oh_encoder = OneHotEncoder(handle_unknown='ignore')
target_encoder = TargetEncoder(smooth="auto")
scaler = MinMaxScaler()

X_train, oh_encoder = one_hot_encode_transform(X_train, oh_encoder, fit=True)
X_train, target_encoder = target_encode_transform(X_train, target_encoder, y=y_train, fit=True)
X_train, scaler = scale_transform(X_train, scaler, fit=True)

X_test, _ = one_hot_encode_transform(X_test, oh_encoder)
X_test, _ = target_encode_transform(X_test,  target_encoder, y=y_test)
X_test, _ = scale_transform(X_test, scaler)


# Training GradientBoostingRegressor
gbr = GradientBoostingRegressor(
        random_state=28, 
        learning_rate=0.1, 
        max_depth=10, 
        min_samples_leaf=5, 
        min_samples_split=2, 
        n_estimators=200
)
gbr.fit(X_train, y_train)


# Training RandomForestRegressor
rfr = RandomForestRegressor(
        random_state=28, 
        max_depth=20, 
        min_samples_leaf=1, 
        min_samples_split=2, 
        n_estimators=100
)
rfr.fit(X_train, y_train)


# Training XGBRegressor
xgbr = XGBRegressor(
        random_state=28, 
        learning_rate=0.05, 
        max_depth=20, 
        subsample=0.7
)
xgbr.fit(X_train, y_train)











testing_results = []

# GradientBoostingRegressor
y_pred_gbr = gbr.predict(X_test)
rmse = root_mean_squared_error(y_test, y_pred_gbr)
r2 = r2_score(y_test, y_pred_gbr)
testing_results.append(["Gradiant Boosting Regressor", rmse, r2])

# RandomForestRegressor
y_pred_rfr = rfr.predict(X_test)
rmse = root_mean_squared_error(y_test, y_pred_rfr)
r2 = r2_score(y_test, y_pred_rfr)
testing_results.append(["Random Forest Regressor", rmse, r2])

# XGBoostRegressor
y_pred_xgbr = xgbr.predict(X_test)
rmse = root_mean_squared_error(y_test, y_pred_xgbr)
r2 = r2_score(y_test, y_pred_xgbr)
testing_results.append(["XGBoost Regressor", rmse, r2])
    
tuned_testing_results_df = pd.DataFrame(testing_results, columns=["Model", "RMSE Score", "R² Score"])
print("80-20 Split Tuned Testing Accuracy")
print(tuned_testing_results_df)











# Improved testing performance after tuning

improved_testing_results_df = pd.DataFrame(data=[], columns=tuned_testing_results_df.columns)
improved_testing_results_df["Model"] = tuned_testing_results_df["Model"]
improved_testing_results_df["RMSE Score"] = tuned_testing_results_df["RMSE Score"].sub(testing_results_df["RMSE Score"])
improved_testing_results_df["R² Score"] = tuned_testing_results_df["R² Score"].sub(testing_results_df["R² Score"])

print("Improved Testing Results after tuning on 80-20 split")
print(improved_testing_results_df)











gbr_importance = pd.DataFrame(gbr.feature_importances_, index=gbr.feature_names_in_)

ax = sns.barplot(gbr_importance, x=gbr_importance[0], y=gbr_importance.index, hue=gbr_importance.index)
for index, value in zip(gbr_importance.index, gbr_importance[0]):
    ax.text(value, index, f"{value*100:.1f}%", color='black', ha='left')
plt.xlabel("Importance")
plt.ylabel("Features")

plt.title("Gradient Boosting Regressor Feature Importance")

plt.savefig('images/gbr_feature_importance.png', bbox_inches = 'tight')
plt.show()








rfr_importance = pd.DataFrame(rfr.feature_importances_, index=rfr.feature_names_in_)

ax = sns.barplot(rfr_importance, x=rfr_importance[0], y=rfr_importance.index, hue=rfr_importance.index)
for index, value in zip(rfr_importance.index, rfr_importance[0]):
    ax.text(value, index, f"{value*100:.1f}%", color='black', ha='left')
plt.xlabel("Importance")
plt.ylabel("Features")

plt.title("Random Forest Regressor Feature Importance")

plt.savefig('images/rfr_feature_importance.png', bbox_inches = 'tight')
plt.show()








xgbr_importance = pd.DataFrame(xgbr.feature_importances_, index=xgbr.feature_names_in_)

ax = sns.barplot(xgbr_importance, x=xgbr_importance[0], y=xgbr_importance.index, hue=xgbr_importance.index)
for index, value in zip(xgbr_importance.index, xgbr_importance[0]):
    ax.text(value, index, f"{value*100:.1f}%", color='black', ha='left')
plt.xlabel("Importance")
plt.ylabel("Features")

plt.title("XGBoost Regressor Feature Importance")

plt.savefig('images/xgbr_feature_importance.png', bbox_inches = 'tight')
plt.show()





















